package com.omega.engine.nn.layer.normalization.gpu;

import static jcuda.driver.JCudaDriver.cuLaunchKernel;

import com.omega.common.data.Tensor;
import com.omega.common.lib.LibPaths;
import com.omega.common.utils.CheckArrayUtils;
import com.omega.common.utils.JsonUtils;
import com.omega.common.utils.MatrixOperation;
import com.omega.common.utils.MatrixUtils;
import com.omega.common.utils.PrintUtils;
import com.omega.common.utils.RandomUtils;
import com.omega.engine.gpu.CUDAMemoryManager;
import com.omega.engine.gpu.CUDAModules;
import com.omega.engine.nn.layer.gpu.BNBaseKernel;
import com.omega.engine.nn.layer.normalization.BNType;
import com.omega.engine.nn.network.RunModel;

import jcuda.Pointer;
import jcuda.Sizeof;
import jcuda.driver.CUdeviceptr;
import jcuda.driver.CUfunction;
import jcuda.driver.JCudaDriver;
import jcuda.runtime.cudaError;

/**
 * mean = batch均值    : 1/n∑xi
 * var = batch方差    : 1/n∑(xi - mean)^2
 * std = sqrt(var + eta)
 * xhati = (xi - mean) / std
 * yi = gama * xhati + beta
 * dgama = ∑delta * xhat
 * dbeta = ∑delta
 * dxhati = gama * deltai
 * dxi = 1 / std * (dxhati - mean(dxhat) - xhati * mean(dxhat * xhat))
 */
public class BNKernel2 extends BNBaseKernel{
	
	private BNType bnType;

	private int C;
	private int H;
	private int W;
	private int meanNumber = 0;
	
	private CUfunction mean_function;
	private CUfunction var_function;
	private CUfunction mwa_function;
	private CUfunction culOutput_function;
	
	/**
	 * fast function
	 */
	private CUfunction fast_mean_function;
	private CUfunction fast_var_function;
	private CUfunction normalize_function;
	private CUfunction normalize_test_function;
	
	private CUfunction computeDiff_function;
	private CUfunction computeDelta_full_function;
	private CUfunction fast_mean_xhat_function;
	private CUfunction fast_mean_dxhat_function;
	private CUfunction mean_xhat_function;
	private CUfunction meanDzSum_function;
	
	/**
	 * fast function
	 */
	private CUfunction dgama_function;
	private CUfunction dbeta_function;
	private CUfunction dxhat_function;
	private CUfunction dx_function;
	private CUfunction dx_full_function;
	
	private int CAFFE_CUDA_NUM_THREADS = 1024;
	
	private float eta = 1e-5f;
	
	private float momentum = 0.01f;
	
	/**
	 * 前向参数
	 */
	private CUdeviceptr d_z;
	private CUdeviceptr d_mean;
	private CUdeviceptr d_var;
	private CUdeviceptr d_std;
	private CUdeviceptr d_runingMean;
	private CUdeviceptr d_runingVar;
	
	/**
	 * 反向参数
	 */
	private CUdeviceptr d_mean_dz;
	private CUdeviceptr d_mean_dzxz;


	/**
	 * 前向方法
	 */
	private Pointer meanParameters;
	private Pointer varParameters;
	private Pointer fastMeanParameters;
	private Pointer fastVarParameters;
	private Pointer normalizeParameters;	
	private Pointer mwaParameters;
	
	/**
	 * 反向方法
	 */
	
	/**
	 * fast function
	 */
	private Pointer dgamaParameters;
	private Pointer dbetaParameters;
	
	private Pointer computeDelta_full_Parameters;
	private Pointer normalize_test_Parameters;
	private Pointer dxhatParameters;
	private Pointer dxParameters;
	private Pointer fast_mean_xhat_Parameters;
	private Pointer fast_mean_dxhat_Parameters;
	private Pointer mean_xhat_Parameters;
	private Pointer dx_fullParameters;

	public BNKernel2(BNType bnType,int C,int H,int W) {
		this.bnType = bnType;
		this.C = C;
		this.H = H;
		this.W = W;
		if(this.bnType == BNType.fully_bn) {
			meanNumber = W;
		}else {
			meanNumber = C;
		}
		init();
	}
	
	public void initFunction() {
		
		try {
			
			if(mean_function == null) {
				mean_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"MathKernel2.cu", "mean_full");
			}
			
			if(fast_mean_function == null) {
				fast_mean_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"MathKernel2.cu", "fast_mean_kernel");
			}
			
			if(var_function == null) {
				var_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"MathKernel2.cu", "var_full");
			}
			
			if(fast_var_function == null) {
				fast_var_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"MathKernel2.cu", "fast_variance_kernel");
			}
			
			if(normalize_function == null) {
				normalize_function =  CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "normalize_kernel");
			}
			
			if(normalize_test_function == null) {
				normalize_test_function =  CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "normalize_test_kernel");
			}
			
			if(mwa_function == null) {
				mwa_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"MathKernel2.cu", "mwa");
			}
			
			if(culOutput_function == null) {
				culOutput_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "culOutput_cov");
			}

			if(computeDelta_full_function == null) {
				computeDelta_full_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "computeDelta_full");
			}
			
			if(meanDzSum_function == null) {
				meanDzSum_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "meanDzSum");
			}
			
			if(computeDiff_function == null) {
				computeDiff_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "computeDiff");
			}
			
			/**
			 * fast function
			 */
			if(dgama_function == null) {
				dgama_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "dgama_kernel");
			}
			
			if(dbeta_function == null) {
				dbeta_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "dbeta_kernel");
			}
			
			if(dxhat_function == null) {
				dxhat_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "dxhat_kernel");
			}
			
			if(dx_function == null) {
				dx_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "dx_kernel");
			}
			
			if(dx_full_function == null) {
				dx_full_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "dx_kernel_full");
			}
			
			if(fast_mean_xhat_function == null) {
				fast_mean_xhat_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "fast_mean_xhat_kernel");
			}
			
			if(mean_xhat_function == null) {
				mean_xhat_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "mean_xhat_kernel");
			}
			
			if(fast_mean_dxhat_function == null) {
				fast_mean_dxhat_function = CUDAModules.getFunctionByModule(LibPaths.LIB_PATH+"BNKernel2.cu", "fast_mean_dxhat_kernel");
			}
			
			
		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
		
	}
	
	private void initKernel() {
		
		/**
		 * 申请向前传播参数显存
		 */
		this.d_mean = CUDAMemoryManager.getDevice(meanNumber);
		this.d_var = CUDAMemoryManager.getDevice(meanNumber);
		this.d_std = CUDAMemoryManager.getDevice(meanNumber);
		this.d_runingMean = CUDAMemoryManager.getDevice(meanNumber);
		this.d_runingVar = CUDAMemoryManager.getDevice(meanNumber);
		this.d_mean_dz = CUDAMemoryManager.getDevice(meanNumber);
		this.d_mean_dzxz = CUDAMemoryManager.getDevice(meanNumber);
		
	}
	
	public void init() {
		/**
		 * 初始化cuda函数
		 */
		initFunction();
		
		/**
		 * 申请显存
		 */
		initKernel();

	}
	
	public void initForward(RunModel RUN_MODEL,Tensor input,Tensor gama,Tensor beta,Tensor output) {
		
		if(input.number != this.N) {
			
			this.N = input.number;
			
			if(bnType == BNType.fully_bn) {

				/**
				 * float* x,float* mean,int number,int width
				 */
				meanParameters = Pointer.to(
		                Pointer.to(input.getGpuData()),
		                Pointer.to(d_mean),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {W})
		            );

				/**
				 * float* x,float* mean,float* var,int number,int width
				 */
				varParameters = Pointer.to(
		                Pointer.to(input.getGpuData()),
		                Pointer.to(d_mean),
		                Pointer.to(d_var),
		                Pointer.to(d_std),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {W})
		            );
				
			}else {

				/**
				 * float *x, int batch, int filters, int spatial, float *mean
				 */
				fastMeanParameters = Pointer.to(
		                Pointer.to(input.getGpuData()),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {C}),
		                Pointer.to(new int[] {H * W}),
		                Pointer.to(d_mean)
		            );
				
				/**
				 * float *x, float *mean, int batch, int filters, int spatial, float *variance
				 */
				fastVarParameters = Pointer.to(
		                Pointer.to(input.getGpuData()),
		                Pointer.to(d_mean),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {C}),
		                Pointer.to(new int[] {H * W}),
		                Pointer.to(d_var),
		                Pointer.to(d_std)
		            );

			}
			
			/**
			 * float* mean,float* var,float* runingMean,float* runingStd,int n
			 */
			mwaParameters = Pointer.to(
	                Pointer.to(d_mean),
	                Pointer.to(d_var),
	                Pointer.to(d_runingMean),
	                Pointer.to(d_runingVar),
	                Pointer.to(new int[] {meanNumber}),
	                Pointer.to(new float[]{momentum})
	            );
			
			int spatial = 1;
			
			if(bnType == BNType.conv_bn) {
				spatial = H * W;
			}

			d_z = CUDAMemoryManager.getDevice(N * C * H * W);

			/**
			 * int N, float *x, float *z, float *out, float *mean, float *variance, float *gama, float *beta,int batch, int filters, int spatial
			 */
			normalizeParameters = Pointer.to(
					Pointer.to(new int[] {N * C * H * W}),
	                Pointer.to(input.getGpuData()),
	                Pointer.to(d_z),
	                Pointer.to(output.getGpuData()),
	                Pointer.to(d_mean),
	                Pointer.to(d_var),
	                Pointer.to(gama.getGpuData()),
	                Pointer.to(beta.getGpuData()),
	                Pointer.to(new int[] {N}),
	                Pointer.to(new int[] {meanNumber}),
	                Pointer.to(new int[] {spatial}),
	                Pointer.to(new float[] {eta})
	            );
			

			/**
			 * int N, float *x, float *z, float *out, float *mean, float *variance, float *gama, float *beta,int batch, int filters, int spatial
			 */
			normalize_test_Parameters = Pointer.to(
					Pointer.to(new int[] {N * C * H * W}),
	                Pointer.to(input.getGpuData()),
	                Pointer.to(d_z),
	                Pointer.to(output.getGpuData()),
	                Pointer.to(d_runingMean),
	                Pointer.to(d_runingVar),
	                Pointer.to(gama.getGpuData()),
	                Pointer.to(beta.getGpuData()),
	                Pointer.to(new int[] {N}),
	                Pointer.to(new int[] {meanNumber}),
	                Pointer.to(new int[] {spatial}),
	                Pointer.to(new float[] {eta})
	            );

		}
		
	}
	
	public void initBackward(Tensor input,Tensor delta,Tensor diff,Tensor gama,Tensor dgama,Tensor dbeta) {
		
		if(dgamaParameters == null) {
		
			if(bnType == BNType.fully_bn) {
				
				/**
				 * 设置入参
				 * float* delta,float* deltaGama,float* deltaBeta,float* z,int number,int width
				 */
				computeDelta_full_Parameters = Pointer.to(
		                Pointer.to(delta.getGpuData()),
		                Pointer.to(dgama.getGpuData()),
		                Pointer.to(dbeta.getGpuData()),
		                Pointer.to(d_z),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {W})
		            );
				
				/**
		         * 设置入参
		         * float *z, float *std, float *diff, float *mean_dz, float *mean_dzxz, int batch, int filters
		         */
				dx_fullParameters = Pointer.to(
		                Pointer.to(d_z),
						Pointer.to(d_std),
						Pointer.to(diff.getGpuData()),
						Pointer.to(d_mean_dz),
						Pointer.to(d_mean_dzxz),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {C})
		            );

			}else {

				/**
		         * 设置入参
		         * int N, float *z, float *std,float *diff, float *mean_dz, float *mean_dzxz, int batch, int filters, int spatial
		         */
				dxParameters = Pointer.to(
						Pointer.to(new int[] {N * C * H * W}),
		                Pointer.to(d_z),
						Pointer.to(d_std),
						Pointer.to(diff.getGpuData()),
						Pointer.to(d_mean_dz),
						Pointer.to(d_mean_dzxz),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {C}),
		                Pointer.to(new int[] {H * W})
		            );
				
				/**
		         * 设置入参
		         * float *x_norm, float *delta, int batch, int c, int size, float *dgama
		         */
				dgamaParameters = Pointer.to(
		                Pointer.to(d_z),
		                Pointer.to(delta.getGpuData()),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {C}),
		                Pointer.to(new int[] {H * W}),
		                Pointer.to(dgama.getGpuData())
		            );
				
				/**
		         * 设置入参
		         * float *dbeta, float *delta, int batch, int c, int size
		         */
				dbetaParameters = Pointer.to(
		                Pointer.to(dbeta.getGpuData()),
		                Pointer.to(delta.getGpuData()),
		                Pointer.to(new int[] {N}),
		                Pointer.to(new int[] {C}),
		                Pointer.to(new int[] {H * W})
		            );

			}
			
			int spatial = 1;
			
			if(bnType == BNType.conv_bn) {
				spatial = H * W;
			}

			/**
			 * float *z,float *dz, int batch, int filters, int spatial, float *mean_dz,float *mean_dzxz
			 */
			fast_mean_xhat_Parameters = Pointer.to(
					Pointer.to(d_z),
	                Pointer.to(diff.getGpuData()),
	                Pointer.to(new int[] {N}),
	                Pointer.to(new int[] {C}),
	                Pointer.to(new int[] {spatial}),
	                Pointer.to(d_mean_dz)
	            );
			
			/**
			 * float *z,float *dz, int batch, int filters, int spatial,float *mean_dzxz
			 */
			fast_mean_dxhat_Parameters = Pointer.to(
					Pointer.to(d_z),
	                Pointer.to(diff.getGpuData()),
	                Pointer.to(new int[] {N}),
	                Pointer.to(new int[] {C}),
	                Pointer.to(new int[] {spatial}),
	                Pointer.to(d_mean_dzxz)
	            );
			
			/**
			 * float *z,float *dz,float *mean_dz,float *mean_dzxz,int number,int channel,int height,int width
			 */
			mean_xhat_Parameters = Pointer.to(
					Pointer.to(d_z),
	                Pointer.to(diff.getGpuData()),
	                Pointer.to(d_mean_dz),
	                Pointer.to(d_mean_dzxz),
	                Pointer.to(new int[] {N}),
	                Pointer.to(new int[] {C}),
	                Pointer.to(new int[] {H}),
	                Pointer.to(new int[] {W})
	            ); 
			
			/**
	         * 设置入参
	         * int N, float *delta, float *dz, float *gama, int filters, int spatial
	         */
			dxhatParameters = Pointer.to(
					Pointer.to(new int[] {N * C * H * W}),
					Pointer.to(delta.getGpuData()),
	                Pointer.to(diff.getGpuData()),
	                Pointer.to(gama.getGpuData()),
	                Pointer.to(new int[] {meanNumber}),
	                Pointer.to(new int[] {spatial})
	            );
			
		}
	}
	
	public int CAFFE_GET_BLOCKS(int N) {
	    return (N + CAFFE_CUDA_NUM_THREADS - 1) / CAFFE_CUDA_NUM_THREADS;
	}
	
	public void forward(RunModel RUN_MODEL, Tensor gama, Tensor beta, Tensor input, Tensor output) {
		
//		long start = System.nanoTime();
		
		initForward(RUN_MODEL, input, gama, beta, output);
		
		if(RUN_MODEL == RunModel.TRAIN) {
			
			/**
			 * 计算标准差
			 * mean = 1/m ∑(x)
			 * var = 1/m ∑(x - mean)^2
			 * std = (var + eta)^1/2
			 */
			if(bnType == BNType.fully_bn){
				
				mean();

				var();

			}else {

				fast_mean();

				fast_var();

			}

			/**
			 * 移动加权平均法计算均值与方差
			 */
			mwa();

			normalize_train(input, gama, beta, output);

		}else {

			normalize_test(input, gama, beta, output);
			
		}

	}
	
	public void mean() {
		
		try {

	        cuLaunchKernel(mean_function,
	        		 this.CAFFE_GET_BLOCKS(meanNumber),  1, 1,      // Grid dimension
		            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
		            0, null,               // Shared memory size and stream
		            meanParameters, null // Kernel- and extra parameters
		        );
	        
//	        JCudaDriver.cuCtxSynchronize();
	        
		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
		
	}

	public void var() {
		
		try {

	        cuLaunchKernel(var_function,
		            this.CAFFE_GET_BLOCKS(meanNumber),  1, 1,      // Grid dimension
		            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
		            0, null,               // Shared memory size and stream
		            varParameters, null // Kernel- and extra parameters
		        );
	        
//	        JCudaDriver.cuCtxSynchronize();
	        
		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
		
	}
	
	public void fast_mean() {
		
		try {

	        cuLaunchKernel(fast_mean_function,
	        		meanNumber,  1, 1,      // Grid dimension
		            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
		            0, null,               // Shared memory size and stream
		            fastMeanParameters, null // Kernel- and extra parameters
		        );
	        
//	        JCudaDriver.cuCtxSynchronize();
	        
		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
		
	}
	
	public void fast_var() {
		
		try {

	        cuLaunchKernel(fast_var_function,
	        		meanNumber,  1, 1,      // Grid dimension
		            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
		            0, null,               // Shared memory size and stream
		            fastVarParameters, null // Kernel- and extra parameters
		        );
	        
//	        JCudaDriver.cuCtxSynchronize();
	        
		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
		
	}
	
	public void normalize_test(Tensor input,Tensor gama, Tensor beta, Tensor output) {
		
		try {
			
	        cuLaunchKernel(normalize_test_function,
		            this.CAFFE_GET_BLOCKS(N * C * H * W),  1, 1,      // Grid dimension
		            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
		            0, null,               // Shared memory size and stream
		            normalize_test_Parameters, null // Kernel- and extra parameters
		        );

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
		
	}

	public void normalize_train(Tensor input,Tensor gama, Tensor beta, Tensor output) {
		
		try {

	        cuLaunchKernel(normalize_function,
		            this.CAFFE_GET_BLOCKS(N * C * H * W),  1, 1,      // Grid dimension
		            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
		            0, null,               // Shared memory size and stream
		            normalizeParameters, null // Kernel- and extra parameters
		        );

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
		
	}
	
	public void mwa() {
		
		try {

	        cuLaunchKernel(mwa_function,
		            this.CAFFE_GET_BLOCKS(meanNumber),  1, 1,      // Grid dimension
		            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
		            0, null,               // Shared memory size and stream
		            mwaParameters, null // Kernel- and extra parameters
		        );
	        
//	        JCudaDriver.cuCtxSynchronize();

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
		
	}
	
	public void backward(Tensor input,Tensor delta,Tensor diff,Tensor gama,Tensor dgama,Tensor dbeta) {

		initBackward(input, delta, diff, gama, dgama, dbeta);
		
		if(bnType == BNType.fully_bn){
			computeDelta_full();
		}else {
			computeDgama();
			computeDbeta();
		}

		computeDxhat();
		
//		computeMeanXhat();
//		
//		computeMeanDXhat();
		
		computeMeanXhat2();
//		
//		System.out.println("d_mean_dz");
//		float[] tmp = new float[C];
//		showDM(d_mean_dz, tmp);
//		
//		System.out.println("d_mean_dzxz");
//		float[] tmp2 = new float[C];
//		showDM(d_mean_dzxz, tmp2);
//		
		if(bnType == BNType.fully_bn) {
			computeDx_full();
		}else {
			computeDx();
		}
		
//		diff.showDM();

	}
	
	private void computeDgama() {
		
		cuLaunchKernel(dgama_function,
				meanNumber,  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            dgamaParameters, null // Kernel- and extra parameters
	        );
		
	}
	
	private void computeDbeta() {
		
		cuLaunchKernel(dbeta_function,
	            meanNumber,  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            dbetaParameters, null // Kernel- and extra parameters
	        );
		
	}
	
	private void computeDelta_full() {
		
		cuLaunchKernel(computeDelta_full_function,
	            this.CAFFE_GET_BLOCKS(meanNumber),  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            computeDelta_full_Parameters, null // Kernel- and extra parameters
	        );
		
//		JCudaDriver.cuCtxSynchronize();
		
	}
	
	private void computeDxhat() {
		
		cuLaunchKernel(dxhat_function,
				this.CAFFE_GET_BLOCKS(N*C*H*W),  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            dxhatParameters, null // Kernel- and extra parameters
	        );
		
//		JCudaDriver.cuCtxSynchronize();
		
	}
	
	private void computeMeanXhat() {
		
		cuLaunchKernel(fast_mean_xhat_function,
				meanNumber,  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            fast_mean_xhat_Parameters, null // Kernel- and extra parameters
	        );
		
//		JCudaDriver.cuCtxSynchronize();
		
	}
	
	private void computeMeanDXhat() {
		
		cuLaunchKernel(fast_mean_dxhat_function,
				meanNumber,  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            fast_mean_dxhat_Parameters, null // Kernel- and extra parameters
	        );
		
//		JCudaDriver.cuCtxSynchronize();
		
	}
	
	private void computeMeanXhat2() {

//		System.out.println("z");
//		float[] tmp1 = new float[N * C * H * W];
//		showDM(d_z, tmp1);
		
		cuLaunchKernel(mean_xhat_function,
				meanNumber,  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            mean_xhat_Parameters, null // Kernel- and extra parameters
	        );
		
//		JCudaDriver.cuCtxSynchronize();
		
	}
	
	private void computeDx() {
		
		cuLaunchKernel(dx_function,
	            this.CAFFE_GET_BLOCKS(N*C*H*W),  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            dxParameters, null // Kernel- and extra parameters
	        );
		
	}
	
	private void computeDx_full() {
		
		cuLaunchKernel(dx_full_function,
	            this.CAFFE_GET_BLOCKS(meanNumber),  1, 1,      // Grid dimension
	            CAFFE_CUDA_NUM_THREADS, 1, 1,      // Block dimension
	            0, null,               // Shared memory size and stream
	            dx_fullParameters, null // Kernel- and extra parameters
	        );
		
	}
	
	public void checkCUDA(int code) {
		if(code != cudaError.cudaSuccess) {
			System.err.println("Error code "+code+":"+cudaError.stringFor(code));
		}
	}

    public void showDM(String id,CUdeviceptr d,float[] data) {
    	JCudaDriver.cuMemcpyDtoH(Pointer.to(data), d, data.length * Sizeof.FLOAT);
    	System.out.println(id + ":"+JsonUtils.toJson(data));
    }
    
    public static void main(String args[]) {
    	
    	CUDAModules.initContext();
    	
    	test2d();
    	
    }
    
    public static void test2d() {
    	int N = 2;
    	int C = 3;
    	int H = 5;
    	int W = 5;
    	
//    	float[] x = RandomUtils.order(N * C * H * W, 1.0f, 1f);
    	
    	float[] x = new float[] {-0.5596f,  0.6154f, -1.1204f, -0.1636f, -1.3229f,
         0.9092f, -0.8235f,  0.3563f,  1.2746f,  0.6454f,
        -0.7569f, -0.6933f, -1.0571f, -1.5361f,  1.8608f,
         0.4835f, -1.3317f, -2.3606f,  0.8470f,  1.1632f,
         0.5080f, -1.4968f,  1.4136f,  0.8903f,  0.4200f,
        0.8233f, -0.6349f,  0.4416f,  0.5081f,  0.1545f,
         0.3967f,  0.6221f, -0.0245f,  0.8493f, -0.8964f,
        -0.5109f, -0.7737f, -0.2255f,  1.6705f,  0.2935f,
         0.5887f, -1.7415f,  0.6597f,  1.2048f,  0.7282f,
         0.8854f,  0.9372f, -0.0824f,  1.6266f, -1.9450f,
       -1.3224f,  0.5002f, -1.0779f,  0.9101f, -0.8541f,
        -0.5130f, -0.4204f,  0.1571f, -0.3905f, -1.3593f,
         1.0415f, -0.9938f,  2.6900f, -1.1995f,  0.7727f,
        -0.2714f,  1.1784f, -0.8269f,  0.3220f,  0.7001f,
        -0.0134f, -1.1899f, -0.8730f,  1.1819f,  0.9492f,
      -1.3036f,  0.3672f, -0.1123f, -0.1105f,  0.4664f,
        -0.7766f, -0.1695f, -1.9371f, -0.4164f,  1.8046f,
        -0.0946f,  0.8305f,  0.9820f,  0.5660f, -0.1472f,
         0.4830f,  1.0134f,  1.3013f, -1.4215f,  0.4570f,
         1.5848f, -0.2974f, -1.2160f,  0.6511f,  0.4922f,
       -1.2987f, -0.9202f, -1.6065f,  0.6146f, -1.7012f,
        -0.5577f,  1.3336f, -0.5391f,  0.1539f, -0.7145f,
         0.2365f,  1.0505f,  0.2315f, -1.4901f,  0.1007f,
         0.7942f, -1.1326f, -1.6860f, -0.0734f,  0.9499f,
         0.2508f,  1.3307f, -0.9660f, -1.3506f, -1.4267f,
        1.1793f, -0.3751f,  0.7723f, -0.2359f, -0.2686f,
         1.2551f, -0.6165f, -1.1625f,  0.5438f, -1.5241f,
        -1.8666f, -0.0040f,  1.6220f, -0.9495f, -0.8307f,
        -2.1322f,  0.3769f,  0.9336f,  0.2458f,  0.1653f,
        -0.4835f, -0.7139f, -0.4338f,  0.1007f,  1.4633f};
    	
    	float[] d = RandomUtils.val(N * C * H * W, 1.0f);
    	
    	float[] g = MatrixUtils.one(C);
    	
    	Tensor input = new Tensor(N, C, H, W, x, true);
    	Tensor gama = new Tensor(1, 1, 1, C, g, true);
    	Tensor beta = new Tensor(1, 1, 1, C, true);
    	
    	Tensor delta = new Tensor(N, C, H, W, d, true);
    	
    	Tensor output = new Tensor(N, C, H, W, true);
    	Tensor diff = new Tensor(N, C, H, W, true);
    	
    	Tensor dgama = new Tensor(1, 1, 1, C, true);
    	Tensor dbeta = new Tensor(1, 1, 1, C, true);
    	
    	BNKernel2 kernel = new BNKernel2(BNType.conv_bn, C, H, W);
    	
    	for(int i = 0;i<1;i++) {

        	kernel.forward(RunModel.TRAIN, gama, beta, input, output);
        	
        	JCudaDriver.cuCtxSynchronize();
        	
        	kernel.backward(input, delta, diff, gama, dgama, dbeta);
        	
    	}
    	
    	output.syncHost();
    	diff.syncHost();
    	
    	PrintUtils.printImage(input.data);
    	
    	System.out.println("");
    	
    	System.out.println("=======output==============");
    	
    	PrintUtils.printImage(output.data);
    	
    	System.out.println("");
    	
    	System.out.println("=======diff==============");
    	
    	PrintUtils.printImage(diff.data);
    	
    	float[][][][] x_cpu = MatrixUtils.transform(x, N, C, H, W);
    	
    	float[][][][] d_cpu = MatrixUtils.transform(d, N, C, H, W);
    	
    	float[][][][] out_cpu = new float[N][C][H][W];
    	
    	float[][][][] diff_cpu = new float[N][C][H][W];
    	
    	float[] dgama_cpu = new float[C];
    	float[] dbeta_cpu = new float[C];
    	
    	kernel.foward_cpu(x_cpu, out_cpu, d_cpu, diff_cpu, gama.data, beta.data, dgama_cpu, dbeta_cpu, 1);
    	
//    	System.out.println("");
    	
//    	PrintUtils.printImage(MatrixUtils.transform(out_cpu));
//    	
//    	System.out.println("==========diff===========");
//    	
//    	PrintUtils.printImage(diff_cpu);
//    	
//
//    	System.out.println("==========gd===========");
//    	
//    	PrintUtils.printImage(dgama_cpu);
//    	
//
//    	System.out.println("==========bd===========");
//    	
//    	PrintUtils.printImage(dbeta_cpu);
    	
    }
    
    public static void test1d() {
    	int N = 2;
    	int C = 1;
    	int H = 1;
    	int W = 10;
    	
//    	float[] x = RandomUtils.order(N * C * H * W, 0.1f, 0.1f);
    	
    	float[] x = new float[] {56.773f,-7.231f,39.634f,24.728f,-17.959f,55.251f,-52.316f,-36.322f,-29.619f,
    			55.24f,26.773f,-1.231f,19.634f,4.728f,7.958f,-65.251f,52.316f,-36.322f,-23.619f,-5.247f};
    	
    	float[] d = RandomUtils.val(N * C * H * W, 1f);
    	
    	float[] g = MatrixUtils.one(W);
    	
    	Tensor input = new Tensor(N, C, H, W, x, true);
    	Tensor gama = new Tensor(1, 1, 1, W, g, true);
    	Tensor beta = new Tensor(1, 1, 1, W, true);
    	
    	Tensor delta = new Tensor(N, C, H, W, d, true);
    	
    	Tensor output = new Tensor(N, C, H, W, true);
    	Tensor diff = new Tensor(N, C, H, W, true);
    	
    	Tensor dgama = new Tensor(1, 1, 1, W, true);
    	Tensor dbeta = new Tensor(1, 1, 1, W, true);
    	
    	BNKernel2 kernel = new BNKernel2(BNType.fully_bn, C, H, W);
    	
    	for(int i = 0;i<1;i++) {

        	kernel.forward(RunModel.TRAIN, gama, beta, input, output);
        	
        	kernel.backward(input, delta, diff, gama, dgama, dbeta);
        	
    	}
    	
    	output.syncHost();
    	diff.syncHost();
    	
    	PrintUtils.printImage(output.data);
    	
    	System.out.println("");
    	
    	System.out.println("=======diff==============");
    	
    	PrintUtils.printImage(diff.data);
    	
//    	float eta = 0.000001f;
//    	
//    	Tensor input1 = new Tensor(N, C, H, W, MatrixOperation.add(x, eta), true);
//    	Tensor input2 = new Tensor(N, C, H, W, MatrixOperation.subtraction(x, eta), true);
//    	Tensor output1 = new Tensor(N, C, H, W, true);
//    	Tensor output2 = new Tensor(N, C, H, W, true);
//    	
//    	
//    	System.out.println("gradientCheck:"+kernel.gradientCheck(input1, input2, dgama, dbeta, output1, output2, diff, eta));
    	
//    	System.out.println(JsonUtils.toJson(output.data));
//    	System.out.println(JsonUtils.toJson(diff.data));
    	
    	float[][][][] x_cpu = MatrixUtils.transform(x, N, C, H, W);
    	
    	float[][][][] d_cpu = MatrixUtils.transform(d, N, C, H, W);
    	
    	float[][][][] out_cpu = new float[N][C][H][W];
    	
    	float[][][][] diff_cpu = new float[N][C][H][W];
    	
    	float[] dgama_cpu = new float[W];
    	float[] dbeta_cpu = new float[W];
    	
    	kernel.foward_cpu(x_cpu, out_cpu, d_cpu, diff_cpu, gama.data, beta.data, dgama_cpu, dbeta_cpu, 0);
    	
    	PrintUtils.printImage(out_cpu);
    	
    	System.out.println("=====================");
    	
    	PrintUtils.printImage(diff_cpu);
    }
    
    public void foward_cpu(float[][][][] x,float[][][][] out,float[][][][] delta,float[][][][] diff,float[] gama,float[] beta,float[] dgama,float[] dbeta,int type) {
    	
    	float[] mean = new float[C];
    	float[] var = new float[C];

    	MatrixOperation.meanV2(x, mean, type);

		/**
		 * 计算标准差
		 * var = 1/m ∑(x - mean)^2
		 */
		
		MatrixOperation.varV2(x, mean, var, type);
		
		/**
		 * std = (var + eta)^1/2
		 * zi = (xi - mean) / (std)
		 */
		float[][][][] z = this.culOutput(x, out, mean, var, gama, beta, type);
		
//		float[] dvar = new float[C];
//
//		float[] dmu = new float[C];
//		
//		computeDelta_cpu(delta, z, gama, dgama, dbeta, diff, type);
		
		/**
		 * 原论文公式
		 * dmean = (∑ dxhat * -1 / (var + eta)^1/2) + dvar * (∑ -2 * (x - mean)) / n
		 * 使用darknet公式
		 * dmean = (∑ dxhat * -1 / (var + eta)^1/2)
		 */
//		meanDzSum_cpu(dvar, mean, var, dmu, x, diff, type);

//		System.out.println("dgama:"+JsonUtils.toJson(dgama));
//		System.out.println("dbeta:"+JsonUtils.toJson(dbeta));
		
//		System.out.println("dgama error:"+CheckArrayUtils.oneCheck(dgama, this.dgama));
//		System.out.println("dbeta error:"+CheckArrayUtils.oneCheck(dbeta, this.dbeta));
//		System.out.println("dgama-cpu:"+JsonUtils.toJson(dgama));
//		System.out.println("dbeta-cpu:"+JsonUtils.toJson(dbeta));
		
//		float[] z_cpu = MatrixUtils.transform(z);
//		float[] z_gpu = new float[z_cpu.length];
		
//		JCudaDriver.cuMemcpyDtoH(Pointer.to(z_gpu), d_z, z_gpu.length * Sizeof.FLOAT);
//		
//		System.out.println("z error:"+CheckArrayUtils.oneCheck(z_cpu, z_gpu));
		
		//float[][][][] diff,float[][][][] x,float[] mean,float[] dmu,float[] std,float[] dvar
//		System.out.println("dmu:");
//		PrintUtils.printImage(dmu);
//		System.out.println("");
//		System.out.println("dvar:");
//		PrintUtils.printImage(dvar);
//		System.out.println("");
//		computeDiff_cpu(diff, x, mean, var, dmu, dvar, type);
////		float[][][][] delta,float[][][][] z,float[][][][] diff,float[] std,float[] gama,float[] dgama,float[] dbeta
////		backward_caffe(delta, z, diff, std, gama, dgama, dbeta);
//		
//		float[][][][] dx = new float[N][C][H][W];
//		
//		float[] std = new float[C];
//		
//		std(x, mean, std);
//		
//		float[] std2 = new float[C];
//		for(int ic = 0;ic<C;ic++) {
//			std2[ic] = (float) Math.sqrt(var[ic] + eta);
//		}
//		
//		for(int ic = 0;ic<C;ic++) {
//			System.out.println(std[ic] + ":" + std2[ic]);
//		}
//		
		float[][][][] dxhat = new float[N][C][H][W];
				
		dxhat(dxhat, gama, delta);
//		
//		dx2(dx, std, z, dxhat);
//		
//		System.out.println("dx2:");
//		
//		PrintUtils.printImage(dx);
//		
//		System.out.println("");
//		
		float[] var2 = new float[C];
		
		var(x, mean, var2);
		
		float[] std3 = new float[C];
		
		for(int ic = 0;ic<C;ic++) {
			std3[ic] = (float) Math.sqrt(var2[ic] + eta);
		}
		
		System.out.println("dx3:");
		
		float[][][][] dx3 = new float[N][C][H][W];
		
		dx3(dx3, std3, z, dxhat);
		
		PrintUtils.printImage(MatrixUtils.transform(dx3));
		
    }
    
    private void computeDelta_cpu(float[][][][] delta,float[][][][] z,float[] gama,float[] dgama,float[] dbeta,float[][][][] diff,int type) {
    	
    	if(type == 1){
//    		System.out.println(C+":"+N + ":" + H + ":" + W);
        	for(int c = 0;c<C;c++) {
    			dgama[c] = 0;
    			dbeta[c] = 0;
    			for(int m = 0;m<N;m++) {
    				for(int h = 0;h<H;h++) {
    					for(int w = 0;w<W;w++) {
    						// deltaGama = ∑ deta * z
    						dgama[c] += delta[m][c][h][w] * z[m][c][h][w];
    						// deltaBeta = ∑ deta
    						dbeta[c] += delta[m][c][h][w];
    						// dxhat = deta * gama
    						diff[m][c][h][w] = delta[m][c][h][w] * gama[c];
    					}
    				}
    			}

    		}
    	}else {

        	for(int w = 0;w<W;w++) {
    			dgama[w] = 0;
    			dbeta[w] = 0;
    			for(int m = 0;m<N;m++) {
    				// deltaGama = ∑ deta * z
					dgama[w] += delta[m][0][0][w] * z[m][0][0][w];
					// deltaBeta = ∑ deta
					dbeta[w] += delta[m][0][0][w];
					// dxhat = deta * gama
					diff[m][0][0][w] = delta[m][0][0][w] * gama[w];
    			}

    		}
    	}
    	
    }
    
    /**
	 * 原论文公式
	 * dvar = ∑ dxhat * (xi - mean) * -1/2 * (var + eta)^-3/2
	 * dmean = (∑ dxhat * -1 / (var + eta)^1/2) + dvar * (∑ -2 * (x - mean)) / n
	 * 使用darknet公式
	 * dvar = ∑ dxhat * (xi - mean) * -1/2 * (var + eta)^-3/2
	 * dmean = (∑ dxhat * -1 / (var + eta)^1/2)
	 */
    private void meanDzSum_cpu(float[] dvar,float[] mean,float[] var,float[] dmu,float[][][][] x,float[][][][] dz,int type) {
    	
    	if(type == 1){
        	for(int c = 0;c<C;c++) {
    			float dvar_val = 0.0f;
    			float dmu_val = 0.0f;
    			for(int n = 0;n<N;n++) {
    				for(int h = 0;h<H;h++) {
    					for(int w = 0;w<W;w++) {
    						dvar_val += (x[n][c][h][w] - mean[c]) * dz[n][c][h][w];
    						dmu_val += -1.0f * dz[n][c][h][w] / (float) Math.sqrt(var[c] + eta);
    					}
    				}
    			}
    			dvar[c] = (float) (dvar_val * -0.5f * Math.pow(var[c] + eta, -1.5));
    			dmu[c] = dmu_val;
    		}
    	}else {
    		for(int w = 0;w<W;w++) {
    			float dvar_val = 0.0f;
    			float dmu_val = 0.0f;
    			for(int n = 0;n<N;n++) {
    				dvar_val += (x[n][0][0][w] - mean[w]) * dz[n][0][0][w];
    				dmu_val += -1.0f * dz[n][0][0][w] / (float) Math.sqrt(var[w] + eta);
    			}
    			dvar[w] = (float) (dvar_val * -0.5f * Math.pow(var[w] + eta, -1.5));
    			dmu[w] = dmu_val;
    		}
    	}
    	
//    	System.out.println("var_cpu:"+JsonUtils.toJson(var));
//    	System.out.println("dmean_cpu:"+JsonUtils.toJson(dmu));
    	
    }
    
    private void computeDiff_cpu(float[][][][] diff,float[][][][] x,float[] mean,float[] var,float[] dmu,float[] dvar,int type) {

    	if(type == 1){

        	float scale = 1.0f / (N * H * W);

        	for(int m = 0;m<N;m++) {
    			for(int c = 0;c<C;c++) {
    				for(int h = 0;h<H;h++) {
    					for(int w = 0;w<W;w++) {
    						
    						// dx = dxhat * 1 / (var + eta)^1/2 + dvar * 2(x - mean) / n + dmean * 1/n
    						diff[m][c][h][w] = diff[m][c][h][w] / (float) Math.sqrt(var[c] + eta) + 2.0f * dvar[c] * (x[m][c][h][w] - mean[c]) * scale + dmu[c] * scale;
    						
    					}
    				}
    			}
    		}
        	
    	}else {

        	float scale = 1.0f / N;

        	for(int m = 0;m<N;m++) {
    			for(int w = 0;w<W;w++) {
    				
    				// dx = dxhat * 1 / (var + eta)^1/2 + dvar * 2(x - mean) / n + dmean * 1/n
    				diff[m][0][0][w] = diff[m][0][0][w] / (float) Math.sqrt(var[w] + eta) + 2.0f * dvar[w] * (x[m][0][0][w] - mean[w]) * scale + dmu[w] * scale;

    			}
    		}
        	
    	}
    	
    }
    
    private float[][][][] culOutput(float[][][][] x,float[][][][] out,float[] m,float[] var,float[] gama,float[] beta, int type) {
		
		int N = x.length;
		int C = x[0].length;
		int H = x[0][0].length;
		int W = x[0][0][0].length;
		
		System.out.println(N+":"+C+":"+H+":"+W);
		
		float[][][][] z = new float[N][C][H][W];
		

		for(int n = 0;n<N;n++) {
			for(int c = 0;c<C;c++) {
				for(int h = 0;h<H;h++) {
					for(int w = 0;w<W;w++) {

						if(type == 0) {
							z[n][c][h][w] = (x[n][c][h][w] - m[w]) / (float) Math.sqrt(var[w] + eta);
							out[n][c][h][w] = z[n][c][h][w] * gama[w] + beta[w];
						}else {
							z[n][c][h][w] = (x[n][c][h][w] - m[c]) / (float) Math.sqrt(var[c] + eta);
							out[n][c][h][w] = z[n][c][h][w] * gama[c] + beta[c];
						}
						
					}
				}
			}
		}
		
		return z;
	}
    
	/**
	 * 
	 * @Title: gradientCheck
	 *
	 * @param x
	 * @return
	 *
	 * @Description:
	 * TODO(这里用一句话描述这个方法的作用)
	 * gradientCheck:
	 * (f(x + eta) - f(x - eta)) / (2 * eta) ≈ f'(x)
	 */
	public float gradientCheck(Tensor input1,Tensor input2,Tensor gama,Tensor beta,Tensor output1,Tensor output2,Tensor diff,float eta) {
		
		this.forward(RunModel.TRAIN, gama, beta, input1, output1);
		
		this.forward(RunModel.TRAIN, gama, beta, input2, output2);
		
		float[] data = MatrixOperation.division(MatrixOperation.subtraction(output1.syncHost(), output2.syncHost()), 2.0f * eta);

		return CheckArrayUtils.check(diff.syncHost(), data);
	}
	
	public void var(float[][][][] x,float[] mean,float[] var) {
		int sn = N * H * W;
		for(int c = 0;c<C;c++) {
			var[c] = 0;
			for(int n = 0;n<N;n++) {
				for(int h = 0;h<H;h++) {
					for(int w = 0;w<W;w++) {
						var[c] += (x[n][c][h][w] - mean[c]) * (x[n][c][h][w] - mean[c]);
					}
				}
			}
			var[c] = var[c] / sn;
		}
	}
	
	public void std(float[][][][] x,float[] mean,float[] std) {
		int sn = N * H * W;
		for(int c = 0;c<C;c++) {
			std[c] = 0;
			for(int n = 0;n<N;n++) {
				for(int h = 0;h<H;h++) {
					for(int w = 0;w<W;w++) {
						std[c] += (x[n][c][h][w] - mean[c]) * (x[n][c][h][w] - mean[c]);
					}
				}
			}
			std[c] = (float) Math.sqrt((std[c]+eta)/sn);
		}
	}
	
	public void dxhat(float[][][][] dxhat,float[] gama,float[][][][] delta) {
		for(int c = 0;c<C;c++) {
			for(int n = 0;n<N;n++) {
				for(int h = 0;h<H;h++) {
					for(int w = 0;w<W;w++) {
						dxhat[n][c][h][w] = delta[n][c][h][w] * gama[c];
					}
				}
			}
		}
	}
	
	/**
	 * std = sqrt(1/n∑(xi-mean)^2)
	 * xhat = (xi - mean) / std
	 * dxhat = gama * delta
	 * dx = 1/(n*std) * (n * dxhat - ∑dxhat - xhat * ∑dxhatj*xhatj)
	 */
	public void dx2(float[][][][] dx,float[] std,float[][][][] xhat,float[][][][] dxhat) {
		
		float[] dxhatSum = new float[C];
		float[] dxhat_Sum = new float[C];
		
		int sn = N * H * W;
		
		/**
		 * ∑dxhatj*xhatj
		 * ∑dxhat
		 */
		for(int c = 0;c<C;c++) {
			for(int n = 0;n<N;n++) {
				for(int h = 0;h<H;h++) {
					for(int w = 0;w<W;w++) {
						dxhat_Sum[c] += dxhat[n][c][h][w] * xhat[n][c][h][w];
						dxhatSum[c] += dxhat[n][c][h][w];
					}
				}
			}
		}
		
		for(int c = 0;c<C;c++) {
			for(int n = 0;n<N;n++) {
				for(int h = 0;h<H;h++) {
					for(int w = 0;w<W;w++) {
						dx[n][c][h][w] = (sn * dxhat[n][c][h][w] - dxhatSum[c] - xhat[n][c][h][w] * dxhat_Sum[c]) / (sn * std[c]);
					}
				}
			}
		}
		
	}
	
	/**
	 * mean = batch均值    : 1/n∑xi
	 * var = batch方差    : 1/n∑(xi - mean)^2
	 * std = sqrt(var + eta)
	 * xhati = (xi - mean) / std
	 * yi = gama * xhati + beta
	 * dgama = ∑delta * xhat
	 * dbeta = ∑delta
	 * dxhati = gama * deltai
	 * dxi = 1 / std * (dxhati - mean(dxhat) - xhati * mean(dxhat * xhat))
	 */
	public void dx3(float[][][][] dx,float[] std,float[][][][] xhat,float[][][][] dxhat) {
		
		float[] dxhatSum = new float[C];
		float[] dxhat_Sum = new float[C];
		
		float[] tmp1 = new float[N * C * H * W];
		showDM(d_z, tmp1);
		
		int sn = N * H * W;
		
		float[][][][] t = MatrixUtils.transform(tmp1, N, C, H, W);

		/**
		 * ∑dxhatj*xhatj
		 * ∑dxhat
		 */
		for(int c = 0;c<C;c++) {
			for(int n = 0;n<N;n++) {
				for(int h = 0;h<H;h++) {
					for(int w = 0;w<W;w++) {
						dxhatSum[c] += dxhat[n][c][h][w];
						dxhat_Sum[c] += dxhat[n][c][h][w] * t[n][c][h][w];
					}
				}
			}
		}
		
		System.out.println(JsonUtils.toJson(MatrixUtils.transform(xhat)));
		
		
		System.out.println(CheckArrayUtils.check(tmp1, MatrixUtils.transform(xhat)));
		

//		System.out.println("dxhatSum:"+JsonUtils.toJson(dxhatSum));
//		
//		System.out.println("dxhat_Sum:"+JsonUtils.toJson(dxhat_Sum));
		
		for(int c = 0;c<C;c++) {
			float m1 = dxhatSum[c] / sn;
			float m2 = dxhat_Sum[c] / sn;
//			System.out.println(m2);
			for(int n = 0;n<N;n++) {
				for(int h = 0;h<H;h++) {
					for(int w = 0;w<W;w++) {
						dx[n][c][h][w] = 1/std[c] * (dxhat[n][c][h][w] - m1 - xhat[n][c][h][w] * m2);
					}
				}
			}
		}
		
	}
    
}

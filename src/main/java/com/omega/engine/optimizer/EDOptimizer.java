package com.omega.engine.optimizer;

import java.math.BigDecimal;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import com.omega.common.data.Tensor;
import com.omega.common.utils.MathUtils;
import com.omega.common.utils.MatrixOperation;
import com.omega.common.utils.MatrixUtils;
import com.omega.common.utils.RandomUtils;
import com.omega.engine.gpu.CUDAModules;
import com.omega.engine.nn.data.BaseData;
import com.omega.engine.nn.grad.GradClipping;
import com.omega.engine.nn.layer.Layer;
import com.omega.engine.nn.layer.gpu.RoPEKernel;
import com.omega.engine.nn.network.ClipVision;
import com.omega.engine.nn.network.GPT;
import com.omega.engine.nn.network.GPT2;
import com.omega.engine.nn.network.Llama2;
import com.omega.engine.nn.network.Llama3;
import com.omega.engine.nn.network.Llava;
import com.omega.engine.nn.network.NanoGPT;
import com.omega.engine.nn.network.Network;
import com.omega.engine.nn.network.RunModel;
import com.omega.engine.nn.network.Seq2Seq;
import com.omega.engine.nn.network.Seq2SeqRNN;
import com.omega.engine.optimizer.lr.LearnRateUpdate;
import com.omega.example.rnn.data.IndexDataLoader;
import com.omega.example.transformer.dataset.JSONDatasetLoader;
import com.omega.example.transformer.dataset.LVMPreTrainDataset;
import com.omega.example.transformer.dataset.PreTrainDataset;
import com.omega.example.transformer.dataset.PreTrainDataset2;
import com.omega.example.transformer.dataset.SFTDataset;
import com.omega.example.transformer.utils.CNChatTokenizer;
import com.omega.example.transformer.utils.CNChatTokenizer2;
import com.omega.example.transformer.utils.CNTokenizer;
import com.omega.example.transformer.utils.CNWikiTokenizer;
import com.omega.example.transformer.utils.CNWikiTokenizer2;
import com.omega.example.transformer.utils.CNWikiTokenizer3;
import com.omega.example.transformer.utils.CNWikiTokenizer4;
import com.omega.example.transformer.utils.ENTokenizer;
import com.omega.example.transformer.utils.ModelUtils;
import com.omega.example.transformer.utils.bpe.CNBpeTokenizer;

import jcuda.driver.JCudaDriver;

public class EDOptimizer extends Optimizer {

//	private float clamp_val = -100;
	
	public EDOptimizer(Network network, int batchSize, int trainTime, float error,LearnRateUpdate learnRateUpdate, boolean warmUp) throws Exception {
		super(network, batchSize, trainTime, error, warmUp);
		this.lr = network.learnRate;
		this.batchSize = batchSize;
		this.trainTime = trainTime;
		this.learnRateUpdate = learnRateUpdate;
		this.loss = new Tensor(batchSize, this.network.oChannel, this.network.oHeight, this.network.oWidth);
		this.lossDiff = new Tensor(batchSize, this.network.oChannel, this.network.oHeight, this.network.oWidth);
	}
	
	public EDOptimizer(Network network, int batchSize, int trainTime, float error, boolean warmUp) throws Exception {
		super(network, batchSize, trainTime, error, warmUp);
		// TODO Auto-generated constructor stub
	}
	
	@Override
	public void train(BaseData trainingData) {
		// TODO Auto-generated method stub
		
	}
	
	public void trainSeq2SeqRNN(IndexDataLoader trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Seq2SeqRNN network = (Seq2SeqRNN) this.network;
			
			Tensor inputEncoder = new Tensor(network.en_time * batchSize, 1, 1, network.en_len, true);

			Tensor inputDecoder = new Tensor(network.de_time * batchSize, 1, 1, network.de_len, true);
			
			Tensor label = new Tensor(network.de_time * batchSize, 1, 1, network.de_len, true);

			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.number,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(indexs[it], inputEncoder, inputDecoder, label);
					
					/**
					 * forward
					 */
					output = network.forward(inputEncoder, inputDecoder);
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label);
					
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label);
					
//					System.out.println(JsonUtils.toJson(output.syncHost()));
					
//					GradClipping.gradClipping(this.lossDiff, 1e-7f);

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.gradClipping(this.network);
					
					/**
					 * update
					 */
					this.network.update();
					
					JCudaDriver.cuCtxSynchronize();
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / inputDecoder.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / inputDecoder.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					float error = this.accuracy(output, label, time, batchSize);
					
//					if(error > 99) {
//						break;
//					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
				}
//				output.showDMByNumber(0);
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainSeq2Seq(IndexDataLoader trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Seq2Seq network = (Seq2Seq) this.network;
			
			Tensor inputEncoder = new Tensor(network.en_time * batchSize, 1, 1, network.en_len, true);

			Tensor inputDecoder = new Tensor(network.de_time * batchSize, 1, 1, network.de_len, true);
			
			Tensor label = new Tensor(network.de_time * batchSize, 1, 1, network.de_len, true);

			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.number,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(indexs[it], inputEncoder, inputDecoder, label);
					
					/**
					 * forward
					 */
					output = network.forward(inputEncoder, inputDecoder);
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label);
					
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label);
					
//					System.out.println(JsonUtils.toJson(output.syncHost()));
					
//					GradClipping.gradClipping(this.lossDiff, 1e-7f);

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.gradClipping(this.network);
					
					/**
					 * update
					 */
					this.network.update();
					
					JCudaDriver.cuCtxSynchronize();
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / inputDecoder.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / inputDecoder.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					float error = this.accuracy(output, label, time, batchSize);
					
//					if(error > 99) {
//						break;
//					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
				}
//				output.showDMByNumber(0);
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainGPT(ENTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			GPT network = (GPT) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, network.vocab_size, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, network.vocab_size, true);
			
			Tensor mask = ENTokenizer.triu(batchSize, network.head_num, network.time, network.time, 1);
			
			Tensor positions = ENTokenizer.getPositions(batchSize, network.time);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.number,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(indexs[it], input, label);
					
					
					/**
					 * forward
					 */
					output = network.forward(input, positions, mask);
//					output.showDMByNumber(0);
//					label.showDMByNumber(0);
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, trainingData.dictionary.get("<pad>"));
//					this.loss = network.loss(output, label);
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, trainingData.dictionary.get("<pad>"));
//					this.lossDiff = network.lossDiff(output, label);
					
//					System.out.println(JsonUtils.toJson(output.syncHost()));
					
//					GradClipping.gradClipping(this.lossDiff, 1e-7f);

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.gradClipping(this.network);
					
					/**
					 * update
					 */
					this.network.update();
					
					JCudaDriver.cuCtxSynchronize();
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, trainingData.dictionary.get("<pad>"));
					
//					if(error > 99) {
//						break;
//					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
				}
//				output.showDMByNumber(0);
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainGPT2(ENTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			GPT2 network = (GPT2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);
			
			Tensor mask = ENTokenizer.triu(batchSize, network.headNum, network.time, network.time, 1);
			
			Tensor positions = ENTokenizer.getPositions(batchSize, network.time);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.number,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(indexs[it], input, label);
					
					
					/**
					 * forward
					 */
					output = network.forward(input, positions, mask);
//					output.showDMByNumber(0);
//					label.showDMByNumber(0);
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, trainingData.dictionary.get("<pad>"));
//					this.loss = network.loss(output, label);
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, trainingData.dictionary.get("<pad>"));
//					this.lossDiff = network.lossDiff(output, label);
					
//					System.out.println(JsonUtils.toJson(output.syncHost()));
					
//					GradClipping.gradClipping(this.lossDiff, 1e-7f);

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.gradClipping(this.network);
					
					this.network.clipGradNorm(1.0f);
					
					/**
					 * update
					 */
					this.network.update();
					
					JCudaDriver.cuCtxSynchronize();
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, trainingData.dictionary.get("<pad>"));
					
//					if(error > 99) {
//						break;
//					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
				}
//				output.showDMByNumber(0);
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainGPT(CNChatTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			GPT network = (GPT) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, network.vocab_size, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, network.vocab_size, true);
			
			Tensor mask = ENTokenizer.triu(batchSize, network.head_num, network.time, network.time, 1);
			
			Tensor positions = ENTokenizer.getPositions(batchSize, network.time);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.number,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(indexs[it], input, label);
					
					
					/**
					 * forward
					 */
					output = network.forward(input, positions, mask);
//					output.showDMByNumber(0);
//					label.showDMByNumber(0);
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, trainingData.dictionary.get("<pad>"));
//					this.loss = network.loss(output, label);
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, trainingData.dictionary.get("<pad>"));
//					this.lossDiff = network.lossDiff(output, label);
//					lossDiff.showDMByNumber(0);
					
//					System.out.println(JsonUtils.toJson(output.syncHost()));
					
//					GradClipping.gradClipping(this.lossDiff, 1e-7f);

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.gradClipping(this.network);
					
					/**
					 * update
					 */
					this.network.update();
					
					JCudaDriver.cuCtxSynchronize();
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, trainingData.dictionary.get("<pad>"));
					
//					if(error > 99) {
//						break;
//					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
				}
//				output.showDMByNumber(0);
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainGPT2(CNChatTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			GPT2 network = (GPT2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);
			
			Tensor mask = ENTokenizer.triu(batchSize, network.headNum, network.time, network.time, 1);
			
			Tensor positions = ENTokenizer.getPositions(batchSize, network.time);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.number,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(indexs[it], input, label);
					
					/**
					 * forward
					 */
					output = network.forward(input, positions, mask);
//					output.showDMByNumber(0);
//					label.showDMByNumber(0);
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, trainingData.dictionary.get("<pad>"));
//					this.loss = network.loss(output, label);
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, trainingData.dictionary.get("<pad>"));
//					this.lossDiff = network.lossDiff(output, label);
//					lossDiff.showDMByNumber(0);
					
//					System.out.println(JsonUtils.toJson(output.syncHost()));
					
//					GradClipping.gradClipping(this.lossDiff, 1e-7f);

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.gradClipping(this.network);
					
					/**
					 * update
					 */
					this.network.update();
					
					JCudaDriver.cuCtxSynchronize();
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, trainingData.dictionary.get("<pad>"));
					
//					if(error > 99) {
//						break;
//					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);
					
					this.batchIndex++;

				}
//				output.showDMByNumber(0);
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainNanoGPT(ENTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			NanoGPT network = (NanoGPT) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);
			
//			Tensor mask = ENTokenizer.triu(batchSize, network.headNum, network.time, network.time, 1);
			
			Tensor positions = ENTokenizer.getPositions(batchSize, network.time);
			
			int itc = new BigDecimal(trainingData.number).divide(new BigDecimal(batchSize), 0, BigDecimal.ROUND_UP).intValue();
			
			int[][] tmp = new int[itc][batchSize];
			
			List<Integer> list = new ArrayList<Integer>(); 
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.number,this.batchSize, tmp, list);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
//					this.loss.clear();
//
//					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(indexs[it], input, label);
					
					
					/**
					 * forward
					 */
					output = network.forward(input, positions);
//					output.showDMByNumber(0);
//					label.showDMByNumber(0);
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, trainingData.dictionary.get("<pad>"));
//					this.loss = network.loss(output, label);
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, trainingData.dictionary.get("<pad>"));
//					this.lossDiff = network.lossDiff(output, label);
					
//					System.out.println(JsonUtils.toJson(output.syncHost()));
					
//					GradClipping.gradClipping(this.lossDiff, 1e-7f);

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.gradClipping(this.network);
					
//					this.network.clipGradNorm(1.0f);
					
					/**
					 * update
					 */
					this.network.update();
					
//					JCudaDriver.cuCtxSynchronize();
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, trainingData.dictionary.get("<pad>"));
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
				}
//				output.showDMByNumber(0);
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainNanoGPT(CNChatTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			NanoGPT network = (NanoGPT) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
//			Tensor mask = CNChatTokenizer.triu(batchSize, network.headNum, network.time, network.time, 1);

			Tensor positions = CNChatTokenizer.getPositions(batchSize, network.time);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				int[][] indexs = MathUtils.randomInts(trainingData.trainData.size(),this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadTrainIdx(indexs[it], input, label);

					/**
					 * forward
					 */
					output = network.forward(input, positions);

					/**
					 * loss
					 */
					this.loss = network.loss(output, label, trainingData.dictionary.get("<pad>"));

					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, trainingData.dictionary.get("<pad>"));

					/**
					 * back
					 */
					this.network.back(this.lossDiff);

					/**
					 * update
					 */
					this.network.update();

					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					if(it % 20 == 0) {
						float error = this.accuracyIdx(input, output, label, time, batchSize, trainingData.vocab, trainingData.dictionary.get("<pad>"));
					}
//					if(error > 99) {
//						break;
//					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
					
//					if(it != 0 && it % 200 == 0) {
//						vail_chat(network, input, output, label, positions, trainingData);
//						network.RUN_MODEL = RunModel.TRAIN;
//					}
					
//					if(it != 0 && it % 200 == 0) {
//						break;
//					}
					
				}
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);

//				output.showDMByNumber(1);
//				
//				OPKernel.getInstance().copy_number_gpu(input, input1, 0, 0);
//				
//				output1 = network.forward(input1, positions1, mask1);
//				output1.showDMByNumber(1);
//				
//				System.out.println("==============>");
//				GPTTest.output2TXT(input1, trainingData, true);
//				GPTTest.output2TXT(output1, trainingData, true);
//				input1.showDMByNumber(0);
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainNanoGPT(CNChatTokenizer2 trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			NanoGPT network = (NanoGPT) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);
			
//			Tensor mask = CNChatTokenizer.triu(batchSize, network.headNum, network.time, network.time, 1);

			Tensor positions = CNChatTokenizer.getPositions(batchSize, network.time);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				int[][] indexs = MathUtils.randomInts(trainingData.trainData.size(),this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadTrainData(indexs[it], input, label);
					
					/**
					 * forward
					 */
					output = network.forward(input, positions);
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, trainingData.tokenizer.specials.get("<pad>"));

					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, trainingData.tokenizer.specials.get("<pad>"));

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.network.clipGradNorm(1.0f);
					
					/**
					 * update
					 */
					this.network.update();

					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();

					int time = output.number / batchSize;
					float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, trainingData.tokenizer.specials.get("<pad>"));

					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
					
				}

				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainNanoGPT_GEN(CNTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			NanoGPT network = (NanoGPT) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);
			
			Tensor positions = CNChatTokenizer.getPositions(batchSize, network.time);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.trainData.length - network.time,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(indexs[it], input, label);
					
					/**
					 * forward
					 */
					output = network.forward(input, positions);
					
					/**
					 * loss
					 */
//					this.loss = network.loss(output, label, trainingData.dictionary.get("<pad>"));
					this.loss = network.loss(output, label);
					/**
					 * loss diff
					 */
//					this.lossDiff = network.lossDiff(output, label, trainingData.dictionary.get("<pad>"));
					this.lossDiff = network.lossDiff(output, label);
//					lossDiff.showDMByNumber(0);
					
//					System.out.println(JsonUtils.toJson(output.syncHost()));
					
//					GradClipping.gradClipping(this.lossDiff, 1e-7f);

					/**
					 * back
					 */
					this.network.back(this.lossDiff);
					
//					/**
//					 * grad clipping
//					 */
//					this.gradClipping(this.network);
					
//					this.network.clipGradNorm(1.0f);
					
					/**
					 * update
					 */
					this.network.update();

					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

//					train_loss += this.currentError;
					
					output.syncHost();
					
//					System.out.println(JsonUtils.toJson(inputEncoder.shape()));
//					System.out.println(JsonUtils.toJson(output.shape()));
//					System.out.println(JsonUtils.toJson(label.shape()));
					int time = output.number / batchSize;
					float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab);
					
//					if(error > 99) {
//						break;
//					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
					
//					if(it != 0 && it % 200 == 0) {
//						vail_gen(network, input, output, label, positions, trainingData);
//						network.RUN_MODEL = RunModel.TRAIN;
//					}
				}
				
//				showOutputAndLabel(trainingData, inputEncoder, output, label, this.batchSize);

				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama2_GEN(CNTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama2 network = (Llama2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.trainData.length - network.time,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadIDXData(indexs[it], input, label);
					
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label);
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label);

					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);

					/**
					 * update
					 */
					this.network.update();

					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();

					int time = output.number / batchSize;
					float error = this.accuracyIdx(input, output, label, time, batchSize, trainingData.vocab);

					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
					
				}
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama2(CNChatTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama2 network = (Llama2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				int[][] indexs = MathUtils.randomInts(trainingData.trainData.size(),this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadTrainIdx(indexs[it], input, label);
					
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, -1);

					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, -1);

					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);

					/**
					 * update
					 */
					this.network.update();

					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();

					int time = output.number / batchSize;
					
					if(it % 20 == 0) {
						float error = this.accuracyIdx(input, output, label, time, batchSize, trainingData.vocab, trainingData.dictionary.get("<pad>"));
					}
					
					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;

				}
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama2_wiki(CNWikiTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama2 network = (Llama2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, network.vocabSize, true);

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			int pad = 0;
			
			if(trainingData.tokenizer != null) {
				pad = trainingData.tokenizer.unk;
			}else {
				pad = trainingData.dictionary.get("<pad>");
			}
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

//				int[][] indexs = MathUtils.randomInts(trainingData.trainData.size(),this.batchSize);
				
				int[][] indexs = MathUtils.sortInts(trainingData.trainData.size(),this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadTrainData(indexs[it], input, label);
					
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);

					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);

					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);

					/**
					 * update
					 */
					this.network.update();

					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();
					
					if(it % 20 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
					
//					if(it != 0 && it % 200 == 0) {
//						vail_chat(network, input, output, label, positions, trainingData);
//						network.RUN_MODEL = RunModel.TRAIN;
//					}

					/**
					 * update learning rate
					 */
					this.updateLR(this.lr_step, it);

				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama2_wiki(CNWikiTokenizer2 trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama2 network = (Llama2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			int pad = -1;
			
			if(trainingData.tokenizer != null) {
				pad = trainingData.tokenizer.pad;
			}else {
				pad = trainingData.dictionary.get("<pad>");
			}
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

//				int[][] indexs = MathUtils.randomInts(trainingData.trainData.size(),this.batchSize);
				
				int[][] indexs = MathUtils.sortInts(trainingData.trainData.size(),this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					this.network.train_time = it + 1;
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
//					JCuda.cudaDeviceSynchronize();
//					long start22 = System.nanoTime();
					/**
					 * 读取训练数据
					 */
					trainingData.loadTrainData(indexs[it], input, label);
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("loadTrainData:"+(System.nanoTime() - start22) / 1e6+"ms.");
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
					
//					JCuda.cudaDeviceSynchronize();
//					long start33 = System.nanoTime();
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);

//					JCuda.cudaDeviceSynchronize();
//					System.out.println("loss:"+(System.nanoTime() - start33) / 1e6+"ms.");

//					long start44 = System.nanoTime();
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("lossDiff:"+(System.nanoTime() - start44) / 1e6+"ms.");
					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);
					
//					JCuda.cudaDeviceSynchronize();
//					long start55 = System.nanoTime();
					/**
					 * update
					 */
					this.network.update();
					
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("update:"+(System.nanoTime() - start55) / 1e6+"ms.");
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();
					
					if(it % 20 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
					
//					if(it != 0 && it % 20 == 0) {
//						break;
//					}

					/**
					 * update learning rate
					 */
					this.updateLR(this.lr_step, it);

				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama2_chinese(CNWikiTokenizer3 trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama2 network = (Llama2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			float[] tmpInput = new float[batchSize * network.time];

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			float[] tmpLabel = new float[batchSize * network.time];

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			int pad = trainingData.tokenizer.pad;
			
			trainingData.loadData(input, label, tmpInput, tmpLabel);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
//				System.out.println(trainingData.count_it);
				for(int it = 0;it<trainingData.count_it;it++) {
					this.network.train_time = it + 1;
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
//					JCuda.cudaDeviceSynchronize();
//					long start22 = System.nanoTime();
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(input, label, tmpInput, tmpLabel);
//					trainingData.loadTrainData(indexs[it], input, label);
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("loadTrainData:"+(System.nanoTime() - start22) / 1e6+"ms.");
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
					
//					JCuda.cudaDeviceSynchronize();
//					long start33 = System.nanoTime();
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);

//					JCuda.cudaDeviceSynchronize();
//					System.out.println("loss:"+(System.nanoTime() - start33) / 1e6+"ms.");

//					long start44 = System.nanoTime();
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("lossDiff:"+(System.nanoTime() - start44) / 1e6+"ms.");
					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);
					
//					JCuda.cudaDeviceSynchronize();
//					long start55 = System.nanoTime();
					/**
					 * update
					 */
					this.network.update();
					
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("update:"+(System.nanoTime() - start55) / 1e6+"ms.");
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();
					
					if(it % 20 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, tmpInput, output, label, tmpLabel, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
					
//					if(it != 0 && it % 20 == 0) {
//						break;
//					}

					/**
					 * update learning rate
					 */
					this.updateLR(this.lr_step, it);

				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama2_chinese(CNWikiTokenizer4 trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama2 network = (Llama2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			float[] tmpInput = new float[batchSize * network.time];

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			float[] tmpLabel = new float[batchSize * network.time];

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			int pad = -1;
			
			trainingData.loadData(input, label, tmpInput, tmpLabel);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
//				System.out.println(trainingData.count_it);
				for(int it = 0;it<trainingData.count_it;it++) {
					this.network.train_time = it + 1;
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
//					JCuda.cudaDeviceSynchronize();
//					long start22 = System.nanoTime();
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(input, label, tmpInput, tmpLabel);
//					trainingData.loadTrainData(indexs[it], input, label);
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("loadTrainData:"+(System.nanoTime() - start22) / 1e6+"ms.");
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
					
//					JCuda.cudaDeviceSynchronize();
//					long start33 = System.nanoTime();
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);

//					JCuda.cudaDeviceSynchronize();
//					System.out.println("loss:"+(System.nanoTime() - start33) / 1e6+"ms.");

//					long start44 = System.nanoTime();
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("lossDiff:"+(System.nanoTime() - start44) / 1e6+"ms.");
					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);
					
//					JCuda.cudaDeviceSynchronize();
//					long start55 = System.nanoTime();
					/**
					 * update
					 */
					this.network.update();
					
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("update:"+(System.nanoTime() - start55) / 1e6+"ms.");
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();
					
					if(it % 20 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, tmpInput, output, label, tmpLabel, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;

					/**
					 * update learning rate
					 */
					this.updateLR(this.lr_step, it);

				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama2_chinese_sft(CNWikiTokenizer4 trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama2 network = (Llama2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			float[] tmpInput = new float[batchSize * network.time];

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			float[] tmpLabel = new float[batchSize * network.time];

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			int pad = trainingData.tokenizer.pad;
			
			trainingData.loadData(input, label, tmpInput, tmpLabel);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
//				System.out.println(trainingData.count_it);
				for(int it = 0;it<trainingData.count_it;it++) {
					this.network.train_time = it + 1;
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
//					JCuda.cudaDeviceSynchronize();
//					long start22 = System.nanoTime();
					/**
					 * 读取训练数据
					 */
					trainingData.loadData(input, label, tmpInput, tmpLabel);
//					trainingData.loadTrainData(indexs[it], input, label);
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("loadTrainData:"+(System.nanoTime() - start22) / 1e6+"ms.");
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
					
//					JCuda.cudaDeviceSynchronize();
//					long start33 = System.nanoTime();
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);

//					JCuda.cudaDeviceSynchronize();
//					System.out.println("loss:"+(System.nanoTime() - start33) / 1e6+"ms.");

//					long start44 = System.nanoTime();
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("lossDiff:"+(System.nanoTime() - start44) / 1e6+"ms.");
					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);
					
//					JCuda.cudaDeviceSynchronize();
//					long start55 = System.nanoTime();
					/**
					 * update
					 */
					this.network.update();
					
//					JCuda.cudaDeviceSynchronize();
//					System.out.println("update:"+(System.nanoTime() - start55) / 1e6+"ms.");
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();
					
					if(it % 20 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, tmpInput, output, label, tmpLabel, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;

					/**
					 * update learning rate
					 */
					this.updateLR(this.lr_step, it);

				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama3_GEN(CNTokenizer trainingData) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama3 network = (Llama3) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);

			Tensor label = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;
				
				int[][] indexs = MathUtils.randomInts(trainingData.trainData.length - network.time,this.batchSize);
				
				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<indexs.length;it++) {
					
					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
					/**
					 * 读取训练数据
					 */
					trainingData.loadIDXData(indexs[it], input, label);
					
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
					
					/**
					 * loss
					 */
					this.loss = network.loss(output, label);
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label);

					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);

					/**
					 * update
					 */
					this.network.update();

					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					output.syncHost();

					int time = output.number / batchSize;
					float error = this.accuracyIdx(input, output, label, time, batchSize, trainingData.vocab);

					String msg = "training["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);

					this.batchIndex++;
					
				}
				
				/**
				 * update learning rate
				 */
				this.updateLR(this.lr_step);

			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama2_chinese(PreTrainDataset trainingData,int gradAccSteps,boolean saveWeight) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama2 network = (Llama2) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			float[] tmpInput = new float[batchSize * network.time];

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			float[] tmpLabel = new float[batchSize * network.time];

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
//			int pad = trainingData.tokenizer.pad;
			
			int pad = -1;
			
			trainingData.loadData2(input, label, tmpInput, tmpLabel, 0);

			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<trainingData.count_it;it++) {

					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();

//					long start22 = System.nanoTime();
					/**
					 * 读取训练数据
					 */
					trainingData.loadData2(input, label, tmpInput, tmpLabel, it);

					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);

//					long start24 = System.nanoTime();
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);
					
//					long start25 = System.nanoTime();
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);

//					long start26 = System.nanoTime();
					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);

					/**
					 * update
					 */
					if(gradAccSteps > 1) {
						this.network.accGrad(gradAccSteps);
					}
					if(it > 1 && it % gradAccSteps == 0) {
						if(this.network.CLIP_GRAD_NORM) {
							this.network.clipGradNorm(1.0f);
						}
						this.network.update();
					}
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					if(it % 100 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"/"+trainingData.count_it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);
					
					/**
					 * dynamic update learnRate
					 */
					updateLRDynamic(i * trainingData.count_it + it, this.trainTime * trainingData.count_it);
					
					this.batchIndex++;
					
					if(saveWeight && it > 1 && it % 20000 == 0) {
						/**
						 * save model
						 */
						String model_path = "/omega/models/llama2-92m-chinese_"+trainIndex+"_"+it+".model";
						
						ModelUtils.saveModel(network, model_path);
					}
					
				}

				if(saveWeight) {
					/**
					 * save model
					 */
					String model_path = "/omega/models/llama2-92m-chinese_"+trainIndex+".model";
					
					ModelUtils.saveModel(network, model_path);
				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void train_llava_chinese(LVMPreTrainDataset trainingData,ClipVision vision,int gradAccSteps,boolean saveWeight) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llava network = (Llava) this.network;
			
			Tensor imageInput = new Tensor(batchSize, 3, trainingData.imageSize, trainingData.imageSize, true);
			
			Tensor indice = new Tensor(batchSize, 1, 1, 1, true);
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
			int pad = trainingData.tokenizer.pad();
			
			trainingData.loadData(input, label, imageInput, indice);
			
			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<trainingData.count_it;it++) {

					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();

					/**
					 * 读取训练数据
					 */
					trainingData.loadData(input, label, imageInput, indice);
					
					/**
					 * vision forward
					 */
					vision.forward(imageInput);
					
					/**
					 * forward
					 */
					output = network.forward(vision.getEncoder().getImageEncoders(), indice, cos, sin, input);
//					System.out.println("forward:"+(System.nanoTime() - start23) / 1e6+"ms.");
					
//					long start24 = System.nanoTime();
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);
					
//					System.out.println("loss:"+(System.nanoTime() - start24) / 1e6+"ms.");
					
//					label.showDM();
					
//					long start25 = System.nanoTime();
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);
//					System.out.println("lossDiff:"+(System.nanoTime() - start25) / 1e6+"ms.");
					
//					long start26 = System.nanoTime();
					/**
					 * back
					 */
					network.back(indice, cos, sin, this.lossDiff);
//					System.out.println("back:"+(System.nanoTime() - start26) / 1e6+"ms.");

					/**
					 * update
					 */
					if(gradAccSteps > 1) {
						this.network.accGrad(gradAccSteps);
					}
					if(it > 1 && it % gradAccSteps == 0) {
						if(this.network.CLIP_GRAD_NORM) {
							this.network.clipGradNorm(1.0f);
						}
						this.network.update();
					}
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						int N = input.number;
						if(pad > -1) {
							N = input.number - MatrixUtils.countOccurrences(label.data, pad);
						}
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / N;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					if(it % 100 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"/"+trainingData.count_it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);
					
					/**
					 * dynamic update learnRate
					 */
					updateLRDynamic(i * trainingData.count_it + it, this.trainTime * trainingData.count_it);
					
					this.batchIndex++;
					
				}

				if(saveWeight) {
					/**
					 * save model
					 */
					String model_path = "/omega/models/llava-26m-chinese_"+trainIndex+".model";
					
					ModelUtils.saveModel(network, model_path);
				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama3_chinese(CNBpeTokenizer trainingData,int gradAccSteps,boolean saveWeight) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama3 network = (Llama3) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			float[] tmpInput = new float[batchSize * network.time];

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			float[] tmpLabel = new float[batchSize * network.time];

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
//			int pad = trainingData.tokenizer.pad;
			
			int pad = -1;
			
			trainingData.loadData2(input, label, tmpInput, tmpLabel, 0);

			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<trainingData.count_it;it++) {

					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();
					
//					JCuda.cudaDeviceSynchronize();
//					long start22 = System.nanoTime();
					/**
					 * 读取训练数据
					 */
					trainingData.loadData2(input, label, tmpInput, tmpLabel, it);
//					input.showDM(0);
//					label.showDM(0);
//					System.out.println("loadTrainData:"+(System.nanoTime() - start22) / 1e6+"ms.");
//					System.out.println(JsonUtils.toJson(tmpInput));
//					System.out.println(JsonUtils.toJson(tmpLabel));
//					long start23 = System.nanoTime();
					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);
//					System.out.println("forward:"+(System.nanoTime() - start23) / 1e6+"ms.");
					
//					long start24 = System.nanoTime();
					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);
					
//					System.out.println("loss:"+(System.nanoTime() - start24) / 1e6+"ms.");
					
//					label.showDM();
					
//					long start25 = System.nanoTime();
					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);
//					System.out.println("lossDiff:"+(System.nanoTime() - start25) / 1e6+"ms.");
					
//					long start26 = System.nanoTime();
					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);
//					System.out.println("back:"+(System.nanoTime() - start26) / 1e6+"ms.");

					/**
					 * update
					 */
					if(gradAccSteps > 1) {
						this.network.accGrad(gradAccSteps);
					}
					if(it > 1 && it % gradAccSteps == 0) {
						if(this.network.CLIP_GRAD_NORM) {
							this.network.clipGradNorm(1.0f);
						}
						this.network.update();
					}
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					if(it % 100 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"/"+trainingData.count_it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);
					
					/**
					 * dynamic update learnRate
					 */
					updateLRDynamic(i * trainingData.count_it + it, this.trainTime * trainingData.count_it);
					
					this.batchIndex++;
					
					if(saveWeight && it > 1 && it % 20000 == 0) {
						/**
						 * save model
						 */
						String model_path = "/omega/models/llama3-26m-chinese_"+trainIndex+"_"+it+".model";
						
						ModelUtils.saveModel(network, model_path);
					}
					
				}

				if(saveWeight) {
					/**
					 * save model
					 */
					String model_path = "/omega/models/llama3-26m-chinese_"+trainIndex+".model";
					
					ModelUtils.saveModel(network, model_path);
				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama3_chinese(PreTrainDataset trainingData,int gradAccSteps,boolean saveWeight) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama3 network = (Llama3) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			float[] tmpInput = new float[batchSize * network.time];

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			float[] tmpLabel = new float[batchSize * network.time];

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];

			int pad = -1;
			
			trainingData.loadData2(input, label, tmpInput, tmpLabel, 0);

			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<trainingData.count_it;it++) {

					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();

					/**
					 * 读取训练数据
					 */
					trainingData.loadData2(input, label, tmpInput, tmpLabel, it);

					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);

					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);

					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);

					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);

					/**
					 * update
					 */
					if(gradAccSteps > 1) {
						this.network.accGrad(gradAccSteps);
					}
					if(it > 1 && it % gradAccSteps == 0) {
						if(this.network.CLIP_GRAD_NORM) {
							this.network.clipGradNorm(1.0f);
						}
						this.network.update();
					}
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					if(it % 100 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"/"+trainingData.count_it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);
					
					/**
					 * dynamic update learnRate
					 */
					updateLRDynamic(i * trainingData.count_it + it, this.trainTime * trainingData.count_it);
					
					this.batchIndex++;
					
					if(saveWeight && it > 1 && it % 20000 == 0) {
						/**
						 * save model
						 */
						String model_path = "/omega/models/llama3-26m-chinese_"+trainIndex+"_"+it+".model";
						
						ModelUtils.saveModel(network, model_path);
					}
					
				}

				if(saveWeight) {
					/**
					 * save model
					 */
					String model_path = "/omega/models/llama3-26m-chinese_"+trainIndex+".model";
					
					ModelUtils.saveModel(network, model_path);
				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama3_chinese(JSONDatasetLoader trainingData,int gradAccSteps,boolean saveWeight,String modelPath) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama3 network = (Llama3) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			float[] tmpInput = new float[batchSize * network.time];

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			float[] tmpLabel = new float[batchSize * network.time];
			
			int[] padCount = new int[] {0};

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];

			int pad = trainingData.tokenizer.pad();
			
			trainingData.loadData(input, label, tmpInput, tmpLabel, padCount, 0);

			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<trainingData.count_it;it++) {

					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();

					/**
					 * 读取训练数据
					 */
					trainingData.loadData(input, label, tmpInput, tmpLabel, padCount, it);

					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);

					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);

					/**
					 * loss diff
					 */
					if(pad >= 0) {
						this.lossDiff = network.lossDiff(output, label, pad, padCount[0]);
					}else {
						this.lossDiff = network.lossDiff(output, label, pad);
					}

					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);

					/**
					 * update
					 */
					if(gradAccSteps > 1) {
						this.network.accGrad(gradAccSteps);
					}
					if(it > 1 && it % gradAccSteps == 0) {
						if(this.network.CLIP_GRAD_NORM) {
							this.network.clipGradNorm(1.0f);
						}
						this.network.update();
					}
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						int count = input.number;
						if(pad >= 0) {
							count = padCount[0];
						}
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / count;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					if(it % 100 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"/"+trainingData.count_it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);
					
					/**
					 * dynamic update learnRate
					 */
					updateLRDynamic(i * trainingData.count_it + it, this.trainTime * trainingData.count_it);
					
					this.batchIndex++;
					
					if(saveWeight && it > 1 && it % 20000 == 0) {
						/**
						 * save model
						 */
						String model_path = modelPath+trainIndex+"_"+it+".model";
						
						ModelUtils.saveModel(network, model_path);
					}
					
				}

				if(saveWeight) {
					/**
					 * save model
					 */
					String model_path = modelPath+"_it:"+trainIndex+".model";
					
					ModelUtils.saveModel(network, model_path);
				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void trainLlama3_chinese_sft(SFTDataset trainingData,int gradAccSteps,boolean saveWeight) {
		// TODO Auto-generated method stub
		try {
			
			CUDAModules.initCUDAFunctions();

			this.dataSize = trainingData.number;
			
			if(isWarmUp()) {
				this.network.learnRate = (float) (this.lr * Math.pow(batchIndex * 1.0f/burnIn * 1.0f, power));
			}
			
			Llama3 network = (Llama3) this.network;
			
			Tensor input = new Tensor(batchSize * network.time, 1, 1, 1, true);
			
			float[] tmpInput = new float[batchSize * network.time];

			Tensor label = new Tensor(batchSize , 1, 1, network.time, true);
			
			float[] tmpLabel = new float[batchSize * network.time];

			Tensor[] cs = RoPEKernel.getCosAndSin(network.time, network.embedDim, network.headNum);
			
			Tensor cos = cs[0];
			
			Tensor sin = cs[1];
			
//			int pad = trainingData.tokenizer.pad();
			int pad = -1;
			
			trainingData.loadData(input, label, tmpInput, tmpLabel, 0);

			for(int i = 0;i<this.trainTime;i++) {
				
				if(this.trainIndex >= this.minTrainTime) {
					break;
				}
				
				this.trainIndex = i + 1;

				Tensor output = null;
				
				/**
				 * 遍历整个训练集
				 */
				for(int it = 0;it<trainingData.count_it;it++) {

					if(Math.abs(this.currentError) <= this.error) {
						break;
					}
					
					long start = System.nanoTime();
					
					this.loss.clear();

					this.lossDiff.clear();

					trainingData.loadData(input, label, tmpInput, tmpLabel, it);

					/**
					 * forward
					 */
					output = network.forward(cos, sin, input);

					/**
					 * loss
					 */
					this.loss = network.loss(output, label, pad);

					/**
					 * loss diff
					 */
					this.lossDiff = network.lossDiff(output, label, pad);

					/**
					 * back
					 */
					network.back(cos, sin, this.lossDiff);

					/**
					 * update
					 */
					if(gradAccSteps > 1) {
						this.network.accGrad(gradAccSteps);
					}
					if(it > 1 && it % gradAccSteps == 0) {
						if(this.network.CLIP_GRAD_NORM) {
							this.network.clipGradNorm(1.0f);
						}
						this.network.update();
					}
					
					/**
					 * current time error
					 */
					if(this.loss.isHasGPU()) {
						int N = input.number;
						if(pad > -1) {
							N = input.number - MatrixUtils.countOccurrences(label.data, pad);
						}
						this.currentError = MatrixOperation.sum(this.loss.syncHost()) / N;
					}else {
						this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
					}

					if(it % 100 == 0) {
						int time = output.number / batchSize;
						if(trainingData.tokenizer != null) {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.tokenizer, pad);
						}else {
							float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, pad);
						}
					}

					String msg = "training["+this.trainIndex+"]{"+it+"/"+trainingData.count_it+"} (lr:"+this.network.learnRate+") train_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
					
					System.out.println(msg);
					
					/**
					 * dynamic update learnRate
					 */
					updateLRDynamic(i * trainingData.count_it + it, this.trainTime * trainingData.count_it);
					
					this.batchIndex++;
					
					if(saveWeight && it > 1 && it % 20000 == 0) {
						/**
						 * save model
						 */
						String model_path = "/omega/models/llama3-26m-chinese_"+trainIndex+"_"+it+".model";
						
						ModelUtils.saveModel(network, model_path);
					}
					
				}

				if(saveWeight) {
					/**
					 * save model
					 */
					String model_path = "/omega/models/llama3-26m-chinese_"+trainIndex+".model";
					
					ModelUtils.saveModel(network, model_path);
				}
				
			}
			
			/**
			 * 停止训练
			 */
			System.out.println("training finish. ["+this.trainIndex+"] finalError:"+this.currentError);

		} catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
		}
	}
	
	public void updateLRDynamic(int it,int count) {
		int warmup_iters = 0;
		int lr_decay_iters = count;
//		System.out.println(this.lr);
//		System.out.println(lr_decay_iters);
	    double min_lr = this.lr / 10.0d;
		
	    if (it < warmup_iters){
	    	network.learnRate = this.lr * it / warmup_iters;
	        return;
	    }
	    if(it > lr_decay_iters) {
	    	network.learnRate = (float) min_lr;
	    	return;
	    }
	    BigDecimal decay_ratio = new BigDecimal(0);
	    
	    if(it > 0) {
	    	decay_ratio = new BigDecimal(it - warmup_iters).divide(new BigDecimal(lr_decay_iters - warmup_iters), 24, BigDecimal.ROUND_HALF_DOWN);
	    }
//	    System.out.println(decay_ratio.doubleValue());
	    
	    BigDecimal coeff = new BigDecimal(0.5d).multiply(new BigDecimal(1).add(new BigDecimal(Math.cos(new BigDecimal(Math.PI).multiply(decay_ratio).doubleValue()))));
	    
	    BigDecimal tlr = new BigDecimal(min_lr).add(coeff.multiply(new BigDecimal((this.lr - min_lr))));
	    tlr = tlr.setScale(24, BigDecimal.ROUND_HALF_DOWN);

	    network.learnRate = (float)tlr.doubleValue();
	}
	
	@Override
	public void train(BaseData trainingData, BaseData testData) {
		// TODO Auto-generated method stub
		
	}

	@Override
	public void train(BaseData trainingData, BaseData validata, BaseData testData) {
		// TODO Auto-generated method stub
		
	}
	
	public void showOutputAndLabel(IndexDataLoader dataLoader,Tensor input,Tensor output,Tensor label,int batchSize) {
		String[] inputTxts = input2TXT(input, dataLoader, batchSize);
		String[] outputTxts = output2TXT(output, dataLoader, batchSize);
		String[] labelTxts = output2TXT(label, dataLoader, batchSize);
		for(int b = 0;b<batchSize;b++) {
			System.out.println("input:"+inputTxts[b]);
			System.out.println("output:"+outputTxts[b]);
			System.out.println("label :"+labelTxts[b]);
		}
	}
	
	public static String[] output2TXT(Tensor output,IndexDataLoader trainData,int batchSize) {
		String[] txts = new String[batchSize];
		for(int b = 0;b<batchSize;b++) {
			String txt = "";
			for(int t = 0;t<trainData.max_ch - 1;t++) {
				int charIndex = pickTopN(output.getByNumber(t * batchSize + b), 1);
				String c = trainData.ch_dic[charIndex];
				txt += c;
			}
			txts[b] = txt;
		}
		return txts;
	}
	
	public static String[] output2TXT(Tensor output,IndexDataLoader trainData,int time,int batchSize) {
		String[] txts = new String[batchSize];
		for(int b = 0;b<batchSize;b++) {
			String txt = "";
			for(int t = 0;t<time;t++) {
				int charIndex = pickTopN(output.getByNumber(t * batchSize + b), 1);
				String c = trainData.ch_dic[charIndex];
				txt += c;
			}
			txts[b] = txt;
		}
		return txts;
	}
	
	public static String[] input2TXT(Tensor input,IndexDataLoader trainData,int batchSize) {
		String[] txts = new String[batchSize];
		for(int b = 0;b<batchSize;b++) {
			String txt = "";
			for(int t = 0;t<trainData.max_en;t++) {
				int charIndex = pickTopN(input.getByNumber(t * batchSize + b), 1);
				String c = trainData.en_dic[charIndex];
				txt += c;
			}
			txts[b] = txt;
		}
		return txts;
	}
	
	public static int pickTopN(float[] x,int n) {

		float[] sort = Arrays.copyOf(x, x.length);
		
		Arrays.sort(sort);
		
		float[] topN = Arrays.copyOfRange(sort, sort.length - n, sort.length);
		
		float v = topN[RandomUtils.getRandomNumber(topN)];
		
		for(int i = 0;i<x.length;i++) {
			if(v == x[i]) {
				return i;
			}
		}
		
		return 0;
	}
	
	public void gradClipping(Network network) {
		
		for(Layer layer:network.layerList) {
			if(layer.diffW != null) {
//				System.out.println(layer.getLayerType()+"-diffW");
				GradClipping.gradClipping(layer.diffW, 1e-7f);
			}
			if(layer.diffB != null) {
//				System.out.println("diffB");
				GradClipping.gradClipping(layer.diffB, 1e-7f);
			}
		}
		
	}
	

	public String predict(IndexDataLoader trainingData,String input_txt) {
		
		Seq2Seq network = (Seq2Seq) this.network;
		
		Tensor input = trainingData.loadByTxt(input_txt);
		
		Tensor[] outputs = network.encoder(input);
		
		Tensor de_hx = outputs[1];
		Tensor de_cx = outputs[2];
		
		float[] data = new float[trainingData.ch_characters];
		data[trainingData.ch_dictionary.get("<BOS>")] = 1.0f;
		Tensor startInput = new Tensor(1, 1, 1, trainingData.ch_characters, data, true);
		
		String txt = "";
		
		for(int t = 0;t<network.de_time;t++) {
			
			Tensor output = network.decoder(de_hx, de_cx, startInput);
			output.syncHost();
			String[] txts = output2TXT(output, trainingData, 1, 1);
			
			txt += txts[0];
			
			startInput.clear();
			startInput.data[trainingData.ch_dictionary.get(txts[0])] = 1.0f;
			startInput.hostToDevice();
			
		}
		System.out.println(txt);
		return txt;
	}
	
	public String predictRNN(IndexDataLoader trainingData,String input_txt) {
		
		Seq2SeqRNN network = (Seq2SeqRNN) this.network;
		
		Tensor input = trainingData.loadByTxt(input_txt);
		
		Tensor en_hidden = network.encoder(input);
		en_hidden.syncHost();
		Tensor de_hidden = new Tensor(1, 1, 1, en_hidden.width, en_hidden.getByNumber(en_hidden.number - 1), true);
		
		float[] data = new float[trainingData.ch_characters];
		data[trainingData.ch_dictionary.get("<BOS>")] = 1.0f;
		Tensor startInput = new Tensor(1, 1, 1, trainingData.ch_characters, data, true);
		
		String txt = "";
		
		for(int t = 0;t<network.de_time;t++) {
			
			Tensor output = network.decoder(de_hidden, startInput);
			output.syncHost();
			String[] txts = output2TXT(output, trainingData, 1, 1);
			
			txt += txts[0];
			
			startInput.clear();
			startInput.data[trainingData.ch_dictionary.get(txts[0])] = 1.0f;
			startInput.hostToDevice();
			
		}
		System.out.println(txt);
		return txt;
	}
	
	public void vail_gen(NanoGPT network,Tensor input,Tensor output,Tensor label,Tensor positions,CNTokenizer trainingData) {
		network.RUN_MODEL = RunModel.TEST;
		int[][] vailIndexs = MathUtils.randomInts(trainingData.vailData.length - network.time,this.batchSize);
		
		for(int it = 0;it<vailIndexs.length;it++) {
			
			if(it > 20) {
				break;
			}
			
			long start = System.nanoTime();
			
			this.loss.clear();

			/**
			 * 读取训练数据
			 */
			trainingData.loadDataVail(vailIndexs[it], input, label);
			/**
			 * forward
			 */
			output = network.forward(input, positions);
			
			/**
			 * loss
			 */
			this.loss = network.loss(output, label);
			
			/**
			 * current time error
			 */
			if(this.loss.isHasGPU()) {
				this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
			}else {
				this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
			}

			output.syncHost();
			int time = output.number / batchSize;
			float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab);
			
			String msg = "vail["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} vail_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
			
			System.out.println(msg);
		}
		
	}
	
	public void vail_chat(NanoGPT network,Tensor input,Tensor output,Tensor label,Tensor positions,CNChatTokenizer trainingData) {
		network.RUN_MODEL = RunModel.TEST;
		int[][] vailIndexs = MathUtils.randomInts(trainingData.vailData.size() - network.time,this.batchSize);
		
		for(int it = 0;it<vailIndexs.length;it++) {
			
			if(it > 20) {
				break;
			}
			
			long start = System.nanoTime();
			
			this.loss.clear();

			/**
			 * 读取训练数据
			 */
			trainingData.loadVailData(vailIndexs[it], input, label);
			/**
			 * forward
			 */
			output = network.forward(input, positions);
			
			/**
			 * loss
			 */
			this.loss = network.loss(output, label, trainingData.dictionary.get("<pad>"));
			
			/**
			 * current time error
			 */
			if(this.loss.isHasGPU()) {
				this.currentError = MatrixOperation.sum(this.loss.syncHost()) / input.number;
			}else {
				this.currentError = MatrixOperation.sum(this.loss.data) / input.number;
			}

			output.syncHost();
			int time = output.number / batchSize;
			float error = this.accuracyBatchFisrt(input, output, label, time, batchSize, trainingData.vocab, trainingData.dictionary.get("<pad>"));
			
			String msg = "vail["+this.trainIndex+"]{"+it+"} (lr:"+this.network.learnRate+") accuracy:{"+error+"%} vail_loss:" + this.currentError + " [costTime:"+(System.nanoTime() - start)/1e6+"ms.]";
			
			System.out.println(msg);
		}
		
	}
	
}
